{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach to be followed - \n",
    "1. Get all 'Ensembl homo_sapiens gene IDs' by performing string manipulation on the gff3 file.\n",
    "2. Pass all the 'gene IDs' (in batches of 100) to REST API to get responses in json format.\n",
    "3. Extract the required data (start, end, strand, ...) from the json response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### -----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use gffpandas python library to load gff3 file and get data in 'pandas dataframe format'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gffpandas.gffpandas as gffpd\n",
    "path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_sequence/Homo_sapiens.GRCh38.96.gff3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajverma/anaconda3/envs/gsoc_ensembl/lib/python3.6/site-packages/gffpandas/gffpandas.py:35: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"attributes\"])\n",
      "/Users/pankajverma/anaconda3/envs/gsoc_ensembl/lib/python3.6/site-packages/ipykernel/__main__.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.58 s, sys: 585 ms, total: 7.17 s\n",
      "Wall time: 7.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "annotation = gffpd.read_gff3(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(annotation.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(annotation.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(annotation.df['attributes'][16:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(annotation.df['attributes'][16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'attributes' column (of the gff3 file) contains all the gene IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation.df['attributes'][16][0:8]\n",
    "gene_ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down below - to get all gene IDs (by performing string manipulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58884\n",
      "CPU times: user 1min 43s, sys: 201 ms, total: 1min 43s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for i in range(len(annotation.df['attributes'])):\n",
    "    if annotation.df['attributes'][i][0:8] == 'ID=gene:':\n",
    "        count = count + 1\n",
    "        gene_ids.append(annotation.df['attributes'][i][8:23])\n",
    "print(count)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids.sort() #arranging the ids in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given below 2 cells for recording the log of responses from API (to keep a record of the error received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "header = ['OK Batch No', 'OK Batch Response']\n",
    "# ok_batch_no = []\n",
    "# ok_batch_response = []\n",
    "# error_batch_no = []\n",
    "# error_batch_response = []\n",
    "ok_response_path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/api_response_record/ok_responses.csv'\n",
    "\n",
    "with open(ok_response_path, 'wt', newline ='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerow(i for i in header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "header = ['Error Batch No', 'Error Batch Response']\n",
    "# ok_batch_no = []\n",
    "# ok_batch_response = []\n",
    "# error_batch_no = []\n",
    "# error_batch_response = []\n",
    "error_response_path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/api_response_record/error_responses.csv'\n",
    "\n",
    "with open(error_response_path, 'wt', newline ='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerow(i for i in header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below cell - saving all gene ids in a csv file, and then loading them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "header = ['SNO', 'All gene ID']\n",
    "\n",
    "path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/all_gene_ids.csv'\n",
    "\n",
    "with open(path, 'wt', newline ='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerow(i for i in header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_no = []\n",
    "for i in range(len(gene_ids)):\n",
    "    s_no.append(i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58884"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df[df.columns[0]] = s_no\n",
    "df[df.columns[1]] = gene_ids\n",
    "\n",
    "df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/all_gene_ids.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "gene_ids = df['All gene ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gene_ids)\n",
    "test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gene_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total gene IDs (for homo_sapiens) are : 58884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58884"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below cell to check if the IDs are in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58884\n",
      "58884\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "d = 0\n",
    "for i in range(len(gene_ids)):\n",
    "    if test[i][0:9] == 'ENSG00000':\n",
    "        c = c + 1\n",
    "    num = test[i][4:16]\n",
    "    num = int(a)\n",
    "    if isinstance(num, int):\n",
    "        d = d + 1\n",
    "print(c)  \n",
    "print(d)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests, sys\n",
    "import json, urllib\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "def csv_to_id(batch):\n",
    "    cleaned = json.dumps(batch)        \n",
    "    correct_format = \"{\" +'\"ids\": ' + cleaned + \"}\"\n",
    "    return correct_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58884"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_ids = []\n",
    "cleaned_ids = gene_ids\n",
    "ok_batch_no = []\n",
    "ok_batch_response = []\n",
    "error_batch_no = []\n",
    "error_batch_response = []\n",
    "len(cleaned_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sending the gene IDs to ensembl API (in batches of 100 [which 1/10th of the assigned limit for post response])\n",
    "### 2. Reason for sending in batches of 100 (although the limit is 1000) - because it was taking a long time for getting data for 1000 IDs. \n",
    "### 3. Total time taken to receive responses for 58884 IDs = ~ 2 hours 45 min (on mac CPU - 2.7 GHz Intel Core i5)\n",
    "### 4. Responses received in 589 json files (having 100 responses in each file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in tqdm(range(0, len(test), 100)):\n",
    "    batch = test[i:i + 100]\n",
    "    cleaned_IDs = csv_to_id(batch)\n",
    "    \n",
    "    server = \"https://rest.ensembl.org\"\n",
    "    ext = '/lookup/id/?format=full;expand=1;utr=1;phenotypes=1'\n",
    "    #ext = '/lookup/id/?\n",
    "    headers = {'Content-Type' : 'application/json', \"Accept\" : 'application/json'}\n",
    "    #'{\"ids\" : [\"ENSG00000255689\", \"ENSG00000254443\"]}'\n",
    "    #cleaned_IDs = {\"ids\": [\"ENSG00000255689\", \"ENSG00000254443\"]}\n",
    "    r = requests.post(server+ext,headers=headers, data='{0}'.format(cleaned_IDs))\n",
    "    #time.sleep(10)\n",
    "#     print(str(r))\n",
    "#     print(type(r))\n",
    "    if str(r) == '<Response [200]>':\n",
    "        df = pd.read_csv(ok_response_path)\n",
    "        ok_batch_no = i\n",
    "        ok_batch_response = str(r)\n",
    "        df2 = df.append(pd.DataFrame([[ok_batch_no, ok_batch_response]], columns=df.columns))\n",
    "        df2.to_csv(ok_response_path, index=False)\n",
    "#         ok_batch_no.append(i)\n",
    "#         ok_batch_response.append(str(r))\n",
    "        \n",
    "    else:\n",
    "        df = pd.read_csv(error_response_path)\n",
    "        error_batch_no = i\n",
    "        error_batch_response = str(r)\n",
    "        df2 = df.append(pd.DataFrame([[error_batch_no, error_batch_response]], columns=df.columns))\n",
    "        df2.to_csv(error_response_path, index=False)\n",
    "        continue\n",
    "#         error_batch_no.append(i)\n",
    "#         error_batch_response.append(str(r))\n",
    "    decoded = r.json()\n",
    "\n",
    "# below 2 lines not advisable since it will take a lot of time for json file to load once it becomes large    \n",
    "#     with open('/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/test_all_gene_data.json', 'a') as outfile:\n",
    "#         json.dump(decoded, outfile, indent=4, sort_keys=True) \n",
    "        \n",
    "    with open('/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/json_all_gene_data(batches)/batch_'+str(i)+'.json', 'w') as outfile:\n",
    "        json.dump(decoded, outfile, indent=4, sort_keys=True) \n",
    "        \n",
    "#print(repr(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the json files into one json file (list format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output_list = []\n",
    "for i in tqdm(range(0,len(test),100)):\n",
    "    \n",
    "    in_path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/json_all_gene_data(batches)/'\n",
    "    batch_json = in_path + 'batch_' + str(i) + '.json'\n",
    "    \n",
    "    with open(batch_json) as infile:\n",
    "        output_list.append(json.load(infile))\n",
    "\n",
    "out_path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/'\n",
    "\n",
    "with open(out_path + \"merged_all_gene_data.json\", \"w\") as outfile:\n",
    "    json.dump(output_list, outfile, indent=4, sort_keys=True)\n",
    "        #json.dump(output_list, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all the json files into one json file (dict format) \n",
    "### Saving in dict format is better for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_1 = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/json_all_gene_data(batches)/batch_0.json'\n",
    "\n",
    "with open(path_1) as infile:\n",
    "    dic1 = json.load(infile)\n",
    "\n",
    "#dic1 = json.load(path_1)\n",
    "for i in tqdm(range(100,len(test),100)):\n",
    "    in_path = '/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/json_all_gene_data(batches)/'\n",
    "    batch_json = in_path + 'batch_' + str(i) + '.json'\n",
    "    \n",
    "    with open(batch_json) as infile:\n",
    "        dic2 = json.load(infile)\n",
    "    #dic2 = json.load(batch_json)\n",
    "    dic1.update(dic2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dumping data into one json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/merged_all_gene_data(dict).json\", \"w\") as outfile:\n",
    "    json.dump(dic1, outfile, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating time taken to open 1.05GB json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 s, sys: 47.2 s, total: 1min 11s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"/Users/pankajverma/Desktop/summerInternship2k17/GSoC/Ensembl/1.lncRNA/dataset/Ensembl_numeric/merged_all_gene_data(dict).json\") as access_json:\n",
    "    read_content = json.load(access_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsoc_ensembl]",
   "language": "python",
   "name": "conda-env-gsoc_ensembl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
